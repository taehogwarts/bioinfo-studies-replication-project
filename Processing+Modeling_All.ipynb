{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6g-72ljH9R7",
        "outputId": "c9b45e2c-7ef1-466e-b839-1e59c68e0122"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 1. Data Preprocessing\n",
        "\n",
        "- 참고: https://github.com/feng-123/Enhancer-LSTMAtt"
      ],
      "metadata": {
        "id": "hlPjWYxsIZR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 검증 가설\n",
        "\n",
        "1. 주어진 DNA sequence의 Enhancer sequence 여부를 NLP 신경망 모델을 통해 판별할 수 있다.\n",
        "2. 주어진 Enhancer sequence를 통해 Strong enhancer인지 Weak enhancer인지 NLP 신경망 모델을 통해 판별할 수 있다."
      ],
      "metadata": {
        "id": "QTiBiocWOZ6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enhancer Sequence Preprocessing\n",
        "\n",
        "Train Dataset\n",
        "- enhancer.cv.txt: Enhancer Sequence 1484개\n",
        "  - strong_742.txt: Strong Enhancer Sequence 742개\n",
        "  - weak_742.txt  : Weak Enhancer Sequence 742개\n",
        "- non.cv.txt     : Non-Enhancer Sequence 1484개\n",
        "\n",
        "Test Dataset\n",
        "- enhancer.ind.txt: Enhancer Sequence 200개\n",
        "  - strong_100.txt : Strong Enhancer Sequence 100개\n",
        "  - weak_100.txt   : Weak Enhancer Sequence 100개\n",
        "- non.ind.txt     : Non-Enhancer Sequence 200개"
      ],
      "metadata": {
        "id": "wB5lvx8lKT6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "7P2i3azgIFLq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 파일 경로 설정\n",
        "\n",
        "train_dir = \"drive/MyDrive/aib-section4-project/dataset/train/\"\n",
        "test_dir = \"drive/MyDrive/aib-section4-project/dataset/test/\""
      ],
      "metadata": {
        "id": "XY_Y4aFhJkPi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 전부 함수로 정의 후 한꺼번에 전처리\n",
        "### 참고 코드: https://github.com/feng-123/Enhancer-LSTMAtt\n",
        "\n",
        "def readSequence():\n",
        "    \n",
        "    ## Enhancer & Non-Enhancer Sets\n",
        "    with open(os.path.join(train_dir, \"enhancer.cv.txt\")) as f:\n",
        "        enhancer_cv = f.readlines()\n",
        "        enhancer_cv = [s.strip() for s in enhancer_cv]\n",
        "    with open(os.path.join(train_dir, \"non.cv.txt\")) as f:\n",
        "        non_cv = f.readlines()\n",
        "        non_cv = [s.strip() for s in non_cv]\n",
        "    with open(os.path.join(test_dir, \"enhancer.ind.txt\")) as f:\n",
        "        enhancer_ind = f.readlines()\n",
        "        enhancer_ind = [s.strip() for s in enhancer_ind]\n",
        "    with open(os.path.join(test_dir, \"non.ind.txt\")) as f:\n",
        "        non_ind = f.readlines()\n",
        "        non_ind = [s.strip() for s in non_ind]\n",
        "    \n",
        "    ## Strong/Weak Enhancer Sets\n",
        "    with open(os.path.join(train_dir, \"strong_742.txt\")) as f:\n",
        "        strong_742 = f.readlines()\n",
        "        strong_742 = [s.strip() for s in strong_742]\n",
        "    with open(os.path.join(train_dir, \"weak_742.txt\")) as f:\n",
        "        weak_742 = f.readlines()\n",
        "        weak_742 = [s.strip() for s in weak_742]\n",
        "    with open(os.path.join(test_dir, \"strong_100.txt\")) as f:\n",
        "        strong_100 = f.readlines()\n",
        "        strong_100 = [s.strip() for s in strong_100]\n",
        "    with open(os.path.join(test_dir, \"weak_100.txt\")) as f:\n",
        "        weak_100 = f.readlines()\n",
        "        weak_100 = [s.strip() for s in weak_100]\n",
        "\n",
        "    return enhancer_cv, non_cv, enhancer_ind, non_ind, strong_742, weak_742, strong_100, weak_100\n",
        "\n",
        "\n",
        "def removeName_PN(data): ## Enhancer(Positive) / Non-Enhancer(Negative) Sequence의 이름 제거\n",
        "    data_new = []\n",
        "    for i in range(1,len(data),2):\n",
        "        data_new.append(data[i])\n",
        "    return data_new\n",
        "\n",
        "def removeName_SW(data): ## Strong/Weak Enhancer Sequence의 이름 제거\n",
        "    data_new = []\n",
        "    for i in range(1,len(data),5):\n",
        "        data_new.append(data[i].upper()+data[i+1].upper()+data[i+2].upper()+data[i+3].upper())\n",
        "    return data_new\n",
        "\n",
        "\n",
        "def GetSets():\n",
        "    enhancer_cv, non_cv, enhancer_ind, non_ind, strong_742, weak_742, strong_100, weak_100 = readSequence()\n",
        "\n",
        "    enhancer_cv = removeName_PN(enhancer_cv)\n",
        "    non_cv = removeName_PN(non_cv)\n",
        "    enhancer_ind = removeName_PN(enhancer_ind)\n",
        "    non_ind = removeName_PN(non_ind)\n",
        "    X_train_pn = np.concatenate([enhancer_cv, non_cv], axis=0)\n",
        "    X_test_pn = np.concatenate([enhancer_ind, non_ind], axis=0)\n",
        "    y_train_pn = np.concatenate([np.ones((len(enhancer_cv),)), np.zeros((len(non_cv),))], axis=0)\n",
        "    y_test_pn = np.concatenate([np.ones((len(enhancer_ind),)), np.zeros((len(non_ind),))], axis=0)\n",
        "    \n",
        "    strong_742 = removeName_SW(strong_742)\n",
        "    weak_742 = removeName_SW(weak_742)\n",
        "    strong_100 = removeName_SW(strong_100)\n",
        "    weak_100 = removeName_SW(weak_100)\n",
        "    X_train_sw = np.concatenate([strong_742, weak_742], axis=0)\n",
        "    X_test_sw = np.concatenate([strong_100, weak_100], axis=0)\n",
        "    y_train_sw = np.concatenate([np.ones((len(strong_742),)), np.zeros((len(weak_742),))], axis=0)\n",
        "    y_test_sw = np.concatenate([np.ones((len(strong_100),)), np.zeros((len(weak_100),))], axis=0)\n",
        "\n",
        "    return X_train_pn, X_test_pn, y_train_pn, y_test_pn, X_train_sw, X_test_sw, y_train_sw, y_test_sw"
      ],
      "metadata": {
        "id": "5oEA4YAEI8OV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pn, X_test_pn, y_train_pn, y_test_pn, X_train_sw, X_test_sw, y_train_sw, y_test_sw = GetSets()"
      ],
      "metadata": {
        "id": "D8QpUdeCJOOI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the Pre-Trained Word2Vec Model: \"dna2vec\"\n",
        "\n",
        "Enhancer Sequence를 벡터화해 주는 사전학습 Word2Vec 모델 불러오기\n",
        "- 출처: https://github.com/pnpnpn/dna2vec\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "***모델 소스코드(6년 전 코드)에서 텐서플로우 구버전 코드 수정***\n",
        "- 코드 수정 레퍼런스\n",
        "  - https://stackoverflow.com/questions/42363897/attributeerror-type-object-word2vec-has-no-attribute-load-word2vec-format\n",
        "  - https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\n",
        "  - https://stackoverflow.com/questions/66868221/gensim-3-8-0-to-gensim-4-0-0\n",
        "  - https://radimrehurek.com/gensim/models/word2vec.html"
      ],
      "metadata": {
        "id": "-j4-ryX0KssA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install logbook"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGF3rvMTLcH-",
        "outputId": "c4753adc-9c2d-49de-d44b-55e462249ac3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: logbook in /usr/local/lib/python3.7/dist-packages (1.5.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6miyeRWxMDZs",
        "outputId": "a28178e2-7581-4e06-ad2e-74176e80b3ec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import logbook\n",
        "import tempfile\n",
        "import numpy as np\n",
        "\n",
        "# from gensim.models import word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim import matutils\n",
        "\n",
        "class SingleKModel:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.vocab_lst = sorted(model.key_to_index.keys())\n",
        "        ###원 코드: self.vocab_lst = sorted(model.vocab.keys())\n",
        "\n",
        "class MultiKModel:\n",
        "    def __init__(self, filepath):\n",
        "        self.aggregate = KeyedVectors.load_word2vec_format(filepath, binary=False)\n",
        "        ###원 코드: self.aggregate = word2vec.Word2Vec.load_word2vec_format(filepath, binary=False)\n",
        "        self.logger = logbook.Logger(self.__class__.__name__)\n",
        "\n",
        "        vocab_lens = [len(vocab) for vocab in self.aggregate.key_to_index.keys()]\n",
        "        ###원 코드: vocab_lens = [len(vocab) for vocab in self.aggregate.vocab.keys()]\n",
        "        self.k_low = min(vocab_lens)\n",
        "        self.k_high = max(vocab_lens)\n",
        "        self.vec_dim = self.aggregate.vector_size\n",
        "\n",
        "        self.data = {}\n",
        "        for k in range(self.k_low, self.k_high + 1):\n",
        "            self.data[k] = self.separate_out_model(k)\n",
        "\n",
        "    def model(self, k_len):\n",
        "        \"\"\"\n",
        "        Use vector('ACGTA') when possible\n",
        "        \"\"\"\n",
        "        return self.data[k_len].model\n",
        "\n",
        "    def vector(self, vocab):\n",
        "        return self.data[len(vocab)].model[vocab]\n",
        "\n",
        "    def unitvec(self, vec):\n",
        "        return matutils.unitvec(vec)\n",
        "\n",
        "    def cosine_distance(self, vocab1, vocab2):\n",
        "        return np.dot(self.unitvec(self.vector(vocab1)), self.unitvec(self.vector(vocab2)))\n",
        "\n",
        "    def l2_norm(self, vocab):\n",
        "        return np.linalg.norm(self.vector(vocab))\n",
        "\n",
        "    def separate_out_model(self, k_len):\n",
        "        vocabs = [vocab for vocab in self.aggregate.key_to_index.keys() if len(vocab) == k_len]\n",
        "        ###원 코드: vocabs = [vocab for vocab in self.aggregate.vocab.keys() if len(vocab) == k_len]\n",
        "        if len(vocabs) != 4 ** k_len:\n",
        "            self.logger.warn('Missing {}-mers: {} / {}'.format(k_len, len(vocabs), 4 ** k_len))\n",
        "\n",
        "        header_str = '{} {}'.format(len(vocabs), self.vec_dim)\n",
        "        with tempfile.NamedTemporaryFile(mode='w') as fptr:\n",
        "            print(header_str, file=fptr)\n",
        "            for vocab in vocabs:\n",
        "                vec_str = ' '.join(\"%f\" % val for val in self.aggregate[vocab])\n",
        "                print('{} {}'.format(vocab, vec_str), file=fptr)\n",
        "            fptr.flush()\n",
        "            return SingleKModel(KeyedVectors.load_word2vec_format(fptr.name, binary=False))\n",
        "            ###원 코드: return SingleKModel(word2vec.Word2Vec.load_word2vec_format(fptr.name, binary=False))"
      ],
      "metadata": {
        "id": "nKwNLzvVK8US"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 모델 불러오기\n",
        "filename = 'dna2vec-20161219-0153-k3to8-100d-10c-29320Mbp-sliding-Xat.w2v'\n",
        "filepath = os.path.join('drive/MyDrive/aib-section4-project/pretrained', filename)\n",
        "\n",
        "### Model Instanciation\n",
        "d2v = MultiKModel(filepath)"
      ],
      "metadata": {
        "id": "f5s1s4ybLiFL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing & Embedding"
      ],
      "metadata": {
        "id": "fN-zF-JrKPFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 모두 함수로 정의\n",
        "\n",
        "def tokenizer(sequence): ## 3-mer로 Tokenize\n",
        "    token_list = []\n",
        "    for i in range(len(sequence)-2): ## 마지막 3-mer까지 얻으려면 길이에서 2뺀 걸 마지막 starting index로 해야 함!\n",
        "        token = sequence[i:i+3]\n",
        "        token_list.append(token)\n",
        "\n",
        "    return token_list\n",
        "\n",
        "\n",
        "def tokenizeSeqs(dataset): ## 데이터셋을 Tokenize\n",
        "    tokenset_list = []\n",
        "    for sequence in dataset:\n",
        "        token_list = tokenizer(sequence)\n",
        "        tokenset_list.append(token_list)\n",
        "\n",
        "    tokenset_array = np.array(tokenset_list) ### array화 - n131 참조\n",
        "    return tokenset_array\n",
        "\n",
        "\n",
        "def embedding(dataset): ## 데이터셋의 각 Token을 Embedding vectors로\n",
        "    data_list = []\n",
        "    for tokenset in dataset:\n",
        "        embedding_list = []\n",
        "        for token in tokenset:\n",
        "            vector = d2v.vector(token)\n",
        "            embedding_list.append(vector)\n",
        "        data_list.append(embedding_list)\n",
        "\n",
        "    dataset_array = np.array(data_list)\n",
        "    return dataset_array"
      ],
      "metadata": {
        "id": "q4Aydl9fKbgA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tokenized_pn = tokenizeSeqs(X_train_pn)\n",
        "X_train_embedded_pn = embedding(X_train_tokenized_pn)\n",
        "\n",
        "X_test_tokenized_pn = tokenizeSeqs(X_test_pn)\n",
        "X_test_embedded_pn = embedding(X_test_tokenized_pn)\n",
        "\n",
        "X_train_tokenized_sw = tokenizeSeqs(X_train_sw)\n",
        "X_train_embedded_sw = embedding(X_train_tokenized_sw)\n",
        "\n",
        "X_test_tokenized_sw = tokenizeSeqs(X_test_sw)\n",
        "X_test_embedded_sw = embedding(X_test_tokenized_sw)"
      ],
      "metadata": {
        "id": "Qz1-BvXHMq4h"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 2. Baseline Model (Chance Level)\n",
        "\n",
        "가설 2개 모두 훈련세트, 테스트세트 각각의 타겟 클래스 비율이 0.5로 동일하게 맞춰져 있음.\n",
        "- 따라서 **Chance Level = 0.5**\n",
        "\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "7tDGJFTIMye_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 3. Loading the Model Architecture of iEnhancer-CNN\n",
        "\n",
        "모델 'iEnhancer-CNN' 논문의 모델 아키텍처를 가져옴.\n",
        "- 논문 링크(open access): https://ieeexplore.ieee.org/abstract/document/9044822\n",
        "- 소스코드는 따로 없으므로 모두 직접 작성\n",
        "\n",
        "***논문에 Flatten layer를 썼다는 이야기는 없으나 output shape를 보면 Dense layer 직전에 필수적인 것으로 보여 추가함.***\n"
      ],
      "metadata": {
        "id": "7MmJXSgyM6VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Conv1D, Dropout, Flatten, Dense, Activation"
      ],
      "metadata": {
        "id": "UgM17V99Nswo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oIWBXcWi4g2A"
      },
      "outputs": [],
      "source": [
        "### 아래에서 GridSearchCV를 사용하기 위해 모델을 함수로 정의\n",
        "\n",
        "def create_model():\n",
        "  input = Input(shape=(198,100))\n",
        "  out = Conv1D(8, 7, strides=1, padding='same')(input)\n",
        "  out = Activation('relu')(out)\n",
        "  out = Conv1D(8, 7, strides=1, padding='same')(out)\n",
        "  out = Activation('relu')(out)\n",
        "  out = Dropout(0.5)(out)\n",
        "  out = Flatten()(out)\n",
        "  output = Dense(1, activation='sigmoid')(out)\n",
        "  \n",
        "  model = Model(inputs=input, outputs=output)\n",
        "  model.compile(\n",
        "      loss='binary_crossentropy', \n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=8*1e-4), \n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxEV4Qh-VbVn",
        "outputId": "f89df74c-26cb-48b1-8b4e-9afd342767f8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 198, 100)]        0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 198, 8)            5608      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 198, 8)            0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 198, 8)            456       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 198, 8)            0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 198, 8)            0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1584)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 1585      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,649\n",
            "Trainable params: 7,649\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 학습 진행해 보기 - 가설1(P/N)\n",
        "### batch+size, epochs는 논문과 같게 설정\n",
        "\n",
        "model.fit(X_train_embedded_pn, y_train_pn, batch_size=32, epochs=50, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9Eqf24JOMc2",
        "outputId": "530b9245-d1d4-4281-fca2-b0ed9fa7a937"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "93/93 [==============================] - 4s 7ms/step - loss: 0.6553 - accuracy: 0.5910\n",
            "Epoch 2/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.5360 - accuracy: 0.7274\n",
            "Epoch 3/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.5148 - accuracy: 0.7500\n",
            "Epoch 4/50\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.5129 - accuracy: 0.7419\n",
            "Epoch 5/50\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 0.5026 - accuracy: 0.7584\n",
            "Epoch 6/50\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.4923 - accuracy: 0.7648\n",
            "Epoch 7/50\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.4844 - accuracy: 0.7679\n",
            "Epoch 8/50\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.4748 - accuracy: 0.7746\n",
            "Epoch 9/50\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 0.4595 - accuracy: 0.7830\n",
            "Epoch 10/50\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.4510 - accuracy: 0.7891\n",
            "Epoch 11/50\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.4464 - accuracy: 0.7901\n",
            "Epoch 12/50\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.4285 - accuracy: 0.8036\n",
            "Epoch 13/50\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.4122 - accuracy: 0.8103\n",
            "Epoch 14/50\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.4075 - accuracy: 0.8184\n",
            "Epoch 15/50\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.3976 - accuracy: 0.8245\n",
            "Epoch 16/50\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 0.3871 - accuracy: 0.8241\n",
            "Epoch 17/50\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.3789 - accuracy: 0.8356\n",
            "Epoch 18/50\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 0.3700 - accuracy: 0.8346\n",
            "Epoch 19/50\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.3560 - accuracy: 0.8386\n",
            "Epoch 20/50\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.3723 - accuracy: 0.8410\n",
            "Epoch 21/50\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.3439 - accuracy: 0.8598\n",
            "Epoch 22/50\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 0.3448 - accuracy: 0.8487\n",
            "Epoch 23/50\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.3357 - accuracy: 0.8558\n",
            "Epoch 24/50\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 0.3284 - accuracy: 0.8602\n",
            "Epoch 25/50\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 0.3254 - accuracy: 0.8544\n",
            "Epoch 26/50\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 0.3348 - accuracy: 0.8544\n",
            "Epoch 27/50\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 0.3161 - accuracy: 0.8666\n",
            "Epoch 28/50\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.3029 - accuracy: 0.8693\n",
            "Epoch 29/50\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 0.2992 - accuracy: 0.8760\n",
            "Epoch 30/50\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.2934 - accuracy: 0.8763\n",
            "Epoch 31/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2921 - accuracy: 0.8716\n",
            "Epoch 32/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2936 - accuracy: 0.8679\n",
            "Epoch 33/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2725 - accuracy: 0.8824\n",
            "Epoch 34/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2744 - accuracy: 0.8895\n",
            "Epoch 35/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2820 - accuracy: 0.8787\n",
            "Epoch 36/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2868 - accuracy: 0.8774\n",
            "Epoch 37/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2715 - accuracy: 0.8821\n",
            "Epoch 38/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2660 - accuracy: 0.8871\n",
            "Epoch 39/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2676 - accuracy: 0.8895\n",
            "Epoch 40/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2604 - accuracy: 0.8908\n",
            "Epoch 41/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2536 - accuracy: 0.8979\n",
            "Epoch 42/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2610 - accuracy: 0.8922\n",
            "Epoch 43/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2409 - accuracy: 0.8952\n",
            "Epoch 44/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2371 - accuracy: 0.8979\n",
            "Epoch 45/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2302 - accuracy: 0.9050\n",
            "Epoch 46/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2363 - accuracy: 0.8976\n",
            "Epoch 47/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2396 - accuracy: 0.8989\n",
            "Epoch 48/50\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.2343 - accuracy: 0.9006\n",
            "Epoch 49/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2270 - accuracy: 0.9023\n",
            "Epoch 50/50\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 0.2292 - accuracy: 0.9057\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff2f4cd3850>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_embedded_pn, y_test_pn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY_QQGjoPCxm",
        "outputId": "07ba8bf9-4587-4b62-afe6-9cf39f029452"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 12ms/step - loss: 0.6341 - accuracy: 0.7625\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.634096086025238, 0.762499988079071]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 학습 진행 - 가설2(S/W)\n",
        "\n",
        "model = create_model()\n",
        "model.fit(X_train_embedded_sw, y_train_sw, batch_size=32, epochs=50, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XYryhJEYvo3",
        "outputId": "6b15f455-0fba-4391-cf5c-e5f2f3f9ab91"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "47/47 [==============================] - 1s 8ms/step - loss: 0.6979 - accuracy: 0.5047\n",
            "Epoch 2/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.6879 - accuracy: 0.5283\n",
            "Epoch 3/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.6776 - accuracy: 0.5761\n",
            "Epoch 4/50\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.6594 - accuracy: 0.6051\n",
            "Epoch 5/50\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.6312 - accuracy: 0.6415\n",
            "Epoch 6/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.6150 - accuracy: 0.6637\n",
            "Epoch 7/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.5936 - accuracy: 0.6813\n",
            "Epoch 8/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.5702 - accuracy: 0.7055\n",
            "Epoch 9/50\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.5553 - accuracy: 0.7230\n",
            "Epoch 10/50\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.5357 - accuracy: 0.7352\n",
            "Epoch 11/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.5224 - accuracy: 0.7392\n",
            "Epoch 12/50\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.4877 - accuracy: 0.7716\n",
            "Epoch 13/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.4710 - accuracy: 0.7951\n",
            "Epoch 14/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.4671 - accuracy: 0.7803\n",
            "Epoch 15/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.4468 - accuracy: 0.7985\n",
            "Epoch 16/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.4258 - accuracy: 0.8154\n",
            "Epoch 17/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.4210 - accuracy: 0.8181\n",
            "Epoch 18/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.4195 - accuracy: 0.8181\n",
            "Epoch 19/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.4021 - accuracy: 0.8295\n",
            "Epoch 20/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.3820 - accuracy: 0.8329\n",
            "Epoch 21/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.3678 - accuracy: 0.8416\n",
            "Epoch 22/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.3688 - accuracy: 0.8416\n",
            "Epoch 23/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.3521 - accuracy: 0.8497\n",
            "Epoch 24/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.3602 - accuracy: 0.8403\n",
            "Epoch 25/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.3527 - accuracy: 0.8430\n",
            "Epoch 26/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.3473 - accuracy: 0.8612\n",
            "Epoch 27/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.3303 - accuracy: 0.8578\n",
            "Epoch 28/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.3035 - accuracy: 0.8827\n",
            "Epoch 29/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.3028 - accuracy: 0.8794\n",
            "Epoch 30/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2952 - accuracy: 0.8740\n",
            "Epoch 31/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2898 - accuracy: 0.8868\n",
            "Epoch 32/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2863 - accuracy: 0.8868\n",
            "Epoch 33/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2756 - accuracy: 0.8969\n",
            "Epoch 34/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2718 - accuracy: 0.8868\n",
            "Epoch 35/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2774 - accuracy: 0.8814\n",
            "Epoch 36/50\n",
            "47/47 [==============================] - 0s 6ms/step - loss: 0.2780 - accuracy: 0.8922\n",
            "Epoch 37/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2510 - accuracy: 0.9077\n",
            "Epoch 38/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2544 - accuracy: 0.8949\n",
            "Epoch 39/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2379 - accuracy: 0.9036\n",
            "Epoch 40/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2501 - accuracy: 0.9009\n",
            "Epoch 41/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2478 - accuracy: 0.8996\n",
            "Epoch 42/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2485 - accuracy: 0.8969\n",
            "Epoch 43/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2212 - accuracy: 0.9178\n",
            "Epoch 44/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2420 - accuracy: 0.9057\n",
            "Epoch 45/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2337 - accuracy: 0.9063\n",
            "Epoch 46/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2464 - accuracy: 0.8962\n",
            "Epoch 47/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2210 - accuracy: 0.9117\n",
            "Epoch 48/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2111 - accuracy: 0.9144\n",
            "Epoch 49/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.1955 - accuracy: 0.9245\n",
            "Epoch 50/50\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.2094 - accuracy: 0.9218\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff2e00a3490>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_embedded_sw, y_test_sw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91mvp2G1ZFS1",
        "outputId": "53e0680a-b017-48a6-a319-35665bae1a24"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 18ms/step - loss: 0.5218 - accuracy: 0.7600\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5218212008476257, 0.7599999904632568]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 4. Fitting the Model by Cross-Validation"
      ],
      "metadata": {
        "id": "yREEIgPCP67k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "실제 학습에서는 Accuracy 외에 논문에서 지정한 다른 Evaluation 파라미터를 사용.\n",
        "- 이미지 링크: https://drive.google.com/file/d/12-fKOkVI2NeYJxbkMr5sFn_PFDj2Hfe5/view\n",
        "- 출처 논문 링크(open access): https://academic.oup.com/bioinformatics/article/32/3/362/1744331"
      ],
      "metadata": {
        "id": "IJFTA6flPsFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 참고 코드: https://github.com/feng-123/Enhancer-LSTMAtt\n",
        "\n",
        "import math\n",
        "\n",
        "def evaluation_metrics(y_true, y_pred): \n",
        "\n",
        "    pos_num = np.sum(y_true==1)\n",
        "    print('pos_num=',pos_num)\n",
        "\n",
        "    neg_num = y_true.shape[0] - pos_num\n",
        "    print('neg_num=',neg_num)\n",
        "\n",
        "    tp =np.sum((y_true==1) & (y_pred==1))\n",
        "    print('tp=',tp)\n",
        "\n",
        "    tn = np.sum(y_true==y_pred) - tp\n",
        "    print('tn=',tn)\n",
        "\n",
        "    sn = tp / pos_num\n",
        "    sp = tn / neg_num\n",
        "\n",
        "    acc = (tp+tn) / (pos_num + neg_num)\n",
        "\n",
        "    fn = pos_num - tp\n",
        "    fp = neg_num - tn\n",
        "    print('fn=',fn)\n",
        "    print('fp=',fp)\n",
        "    \n",
        "    tp = np.array(tp)\n",
        "    tn = np.array(tn)\n",
        "    fp = np.array(fp)\n",
        "    fn = np.array(fn)\n",
        "    mcc = (tp*tn - fp*fn) / (np.sqrt((tp+fn)*(tp+fp)*(tn+fp)*(tn+fn)))\n",
        "\n",
        "    return sn, sp, acc, mcc"
      ],
      "metadata": {
        "id": "7N5mNR5fRZJv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Fold Cross-Validation w/ GridSearch\n",
        "\n",
        "논문에서 Layer의 파라미터는 다 최적화한 상태이므로 batch size만 튜닝하면서 K-Fold CV를 함께 진행.\n",
        "- 논문처럼 k=5로 설정."
      ],
      "metadata": {
        "id": "JKnbpiU2Q8Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "metadata": {
        "id": "JSVGE1DrUwMb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
        "\n",
        "batch_size = [8, 16, 32, 64]\n",
        "epochs = [50]\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l57m2RUBVjhE",
        "outputId": "05058a37-13ce-4173-c9b3-d391e8f81559"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis 1"
      ],
      "metadata": {
        "id": "iUlO9mqUrKB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 가설1(P/N)\n",
        "\n",
        "cv = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, verbose=1, n_jobs=-1)\n",
        "cv_pn_result = cv.fit(X_train_embedded_pn, y_train_pn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVtLtbU2W90Q",
        "outputId": "32714993-ec77-4946-bc12-bc1a3b61e525"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "371/371 [==============================] - 2s 4ms/step - loss: 0.5915 - accuracy: 0.6691\n",
            "Epoch 2/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7476\n",
            "Epoch 3/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.5061 - accuracy: 0.7554\n",
            "Epoch 4/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.4868 - accuracy: 0.7685\n",
            "Epoch 5/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.4727 - accuracy: 0.7793\n",
            "Epoch 6/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.4527 - accuracy: 0.7867\n",
            "Epoch 7/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.4388 - accuracy: 0.8076\n",
            "Epoch 8/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.4232 - accuracy: 0.8083\n",
            "Epoch 9/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.3931 - accuracy: 0.8221\n",
            "Epoch 10/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.3931 - accuracy: 0.8221\n",
            "Epoch 11/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.3630 - accuracy: 0.8369\n",
            "Epoch 12/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.3555 - accuracy: 0.8460\n",
            "Epoch 13/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.3476 - accuracy: 0.8470\n",
            "Epoch 14/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.3423 - accuracy: 0.8477\n",
            "Epoch 15/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.3148 - accuracy: 0.8582\n",
            "Epoch 16/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.3101 - accuracy: 0.8656\n",
            "Epoch 17/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.3227 - accuracy: 0.8565\n",
            "Epoch 18/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2985 - accuracy: 0.8747\n",
            "Epoch 19/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2969 - accuracy: 0.8683\n",
            "Epoch 20/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2928 - accuracy: 0.8770\n",
            "Epoch 21/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2921 - accuracy: 0.8713\n",
            "Epoch 22/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2672 - accuracy: 0.8827\n",
            "Epoch 23/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2732 - accuracy: 0.8848\n",
            "Epoch 24/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2675 - accuracy: 0.8844\n",
            "Epoch 25/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2660 - accuracy: 0.8905\n",
            "Epoch 26/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2525 - accuracy: 0.8932\n",
            "Epoch 27/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2455 - accuracy: 0.8942\n",
            "Epoch 28/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2386 - accuracy: 0.8966\n",
            "Epoch 29/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2516 - accuracy: 0.8952\n",
            "Epoch 30/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2274 - accuracy: 0.9053\n",
            "Epoch 31/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2412 - accuracy: 0.8979\n",
            "Epoch 32/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2345 - accuracy: 0.9003\n",
            "Epoch 33/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2398 - accuracy: 0.8993\n",
            "Epoch 34/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2368 - accuracy: 0.9046\n",
            "Epoch 35/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2252 - accuracy: 0.9090\n",
            "Epoch 36/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2070 - accuracy: 0.9151\n",
            "Epoch 37/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2195 - accuracy: 0.9067\n",
            "Epoch 38/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2237 - accuracy: 0.8993\n",
            "Epoch 39/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2044 - accuracy: 0.9191\n",
            "Epoch 40/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2273 - accuracy: 0.9077\n",
            "Epoch 41/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2062 - accuracy: 0.9137\n",
            "Epoch 42/50\n",
            "371/371 [==============================] - 2s 6ms/step - loss: 0.2146 - accuracy: 0.9090\n",
            "Epoch 43/50\n",
            "371/371 [==============================] - 2s 4ms/step - loss: 0.2115 - accuracy: 0.9144\n",
            "Epoch 44/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2222 - accuracy: 0.9046\n",
            "Epoch 45/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.1995 - accuracy: 0.9158\n",
            "Epoch 46/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.2086 - accuracy: 0.9121\n",
            "Epoch 47/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.1948 - accuracy: 0.9195\n",
            "Epoch 48/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.1778 - accuracy: 0.9262\n",
            "Epoch 49/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.1882 - accuracy: 0.9218\n",
            "Epoch 50/50\n",
            "371/371 [==============================] - 1s 3ms/step - loss: 0.1882 - accuracy: 0.9232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best: {cv_pn_result.best_score_} using {cv_pn_result.best_params_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iasXlpbHa982",
        "outputId": "819df5b6-6bfb-4708-9b68-eb93f394be56"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.6320949733257294 using {'batch_size': 8, 'epochs': 50}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis 2"
      ],
      "metadata": {
        "id": "-fIVa-w9rNyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 가설2(S/W)\n",
        "\n",
        "cv2 = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "cv_sw_result = cv2.fit(X_train_embedded_sw, y_train_sw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euOWRvxka5EO",
        "outputId": "e321bcf4-ed90-496a-853a-279450caeb6d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.6963 - accuracy: 0.5020\n",
            "Epoch 2/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.6756 - accuracy: 0.5869\n",
            "Epoch 3/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.6400 - accuracy: 0.6368\n",
            "Epoch 4/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.5998 - accuracy: 0.6806\n",
            "Epoch 5/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.5756 - accuracy: 0.7008\n",
            "Epoch 6/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.5397 - accuracy: 0.7352\n",
            "Epoch 7/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.4980 - accuracy: 0.7581\n",
            "Epoch 8/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.4862 - accuracy: 0.7547\n",
            "Epoch 9/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.4542 - accuracy: 0.7864\n",
            "Epoch 10/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.4250 - accuracy: 0.8005\n",
            "Epoch 11/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.4049 - accuracy: 0.8208\n",
            "Epoch 12/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.3916 - accuracy: 0.8336\n",
            "Epoch 13/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.3428 - accuracy: 0.8518\n",
            "Epoch 14/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.3485 - accuracy: 0.8585\n",
            "Epoch 15/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.3413 - accuracy: 0.8457\n",
            "Epoch 16/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.3116 - accuracy: 0.8693\n",
            "Epoch 17/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2923 - accuracy: 0.8780\n",
            "Epoch 18/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2983 - accuracy: 0.8699\n",
            "Epoch 19/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2862 - accuracy: 0.8929\n",
            "Epoch 20/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2996 - accuracy: 0.8726\n",
            "Epoch 21/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2719 - accuracy: 0.8908\n",
            "Epoch 22/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2465 - accuracy: 0.8922\n",
            "Epoch 23/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2848 - accuracy: 0.8848\n",
            "Epoch 24/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2414 - accuracy: 0.9030\n",
            "Epoch 25/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2459 - accuracy: 0.9030\n",
            "Epoch 26/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2210 - accuracy: 0.9131\n",
            "Epoch 27/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2337 - accuracy: 0.9050\n",
            "Epoch 28/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2339 - accuracy: 0.9030\n",
            "Epoch 29/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2242 - accuracy: 0.9137\n",
            "Epoch 30/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2135 - accuracy: 0.9212\n",
            "Epoch 31/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2116 - accuracy: 0.9185\n",
            "Epoch 32/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2101 - accuracy: 0.9151\n",
            "Epoch 33/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.1888 - accuracy: 0.9272\n",
            "Epoch 34/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2031 - accuracy: 0.9239\n",
            "Epoch 35/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.1717 - accuracy: 0.9373\n",
            "Epoch 36/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.1959 - accuracy: 0.9158\n",
            "Epoch 37/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.1820 - accuracy: 0.9259\n",
            "Epoch 38/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2004 - accuracy: 0.9104\n",
            "Epoch 39/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.1826 - accuracy: 0.9232\n",
            "Epoch 40/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.1776 - accuracy: 0.9306\n",
            "Epoch 41/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.1791 - accuracy: 0.9313\n",
            "Epoch 42/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.1803 - accuracy: 0.9272\n",
            "Epoch 43/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.1759 - accuracy: 0.9299\n",
            "Epoch 44/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.1609 - accuracy: 0.9319\n",
            "Epoch 45/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.1653 - accuracy: 0.9313\n",
            "Epoch 46/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.1669 - accuracy: 0.9313\n",
            "Epoch 47/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.1832 - accuracy: 0.9292\n",
            "Epoch 48/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.1445 - accuracy: 0.9407\n",
            "Epoch 49/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.1611 - accuracy: 0.9367\n",
            "Epoch 50/50\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.1632 - accuracy: 0.9394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best: {cv_sw_result.best_score_} using {cv_sw_result.best_params_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYp1wXwsa5j_",
        "outputId": "5932837e-054c-49f5-d8e8-b5235ea45874"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.44129583835601804 using {'batch_size': 8, 'epochs': 50}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_pn = cv.best_estimator_\n",
        "model_sw = cv2.best_estimator_"
      ],
      "metadata": {
        "id": "5aeHhTJ0bVJa"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 5. Testing the Model"
      ],
      "metadata": {
        "id": "sRb_1GI8bdTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "0QNZN1VBltzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc"
      ],
      "metadata": {
        "id": "EDInXo1Jma3H"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ROC Curve로 결과를 시각화하는 함수 정의\n",
        "\n",
        "def plot_roc_curve(y_true, y_pred):\n",
        "  fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "  auc = auc(fpr, tpr)\n",
        "  \n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.title('ROC curves')\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.xlim([-0.05,1.05])\n",
        "  plt.ylim([-0.05,1.05])\n",
        "  plt.plot(fpr, tpr, color='r')\n",
        "  plt.plot([0, 1], [0, 1], color='m', linestyle='--')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Q08RbP4IlxPE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypothesis 1. Enhancer vs. Non-Enhancer"
      ],
      "metadata": {
        "id": "DDQcoNGLiK2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_pn = model_pn.predict(X_test_embedded_pn)\n",
        "### predict array shape이 (400, 1)로 레이블마다 []가 들어가므로 reshape\n",
        "\n",
        "y_pred_pn = y_pred_pn.reshape((400,))"
      ],
      "metadata": {
        "id": "y_bj9MnKiJJq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn1, sp1, acc1, mcc1 = evaluation_metrics(y_test_pn, y_pred_pn)\n",
        "print(f\"sn={sn1}\")\n",
        "print(f\"sp={sp1}\")\n",
        "print(f\"acc={acc1}\")\n",
        "print(f\"mcc={mcc1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Weu61g-llE_Z",
        "outputId": "e9f18ed8-71cc-4831-bacd-eb06f6024762"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos_num= 200\n",
            "neg_num= 200\n",
            "tp= 167\n",
            "tn= 140\n",
            "fn= 33\n",
            "fp= 60\n",
            "sn=0.835\n",
            "sp=0.7\n",
            "acc=0.7675\n",
            "mcc=0.539942853687922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_roc_curve(y_test_pn, y_pred_pn)\n",
        "\n",
        "### 시각화하려 했으나 RecursionError가 남.\n",
        "### https://stackoverflow.com/questions/3323001/what-is-the-maximum-recursion-depth-in-python-and-how-to-increase-it\n",
        "\n",
        "### 위 페이지 참고해서 recursion limit을 조금씩 올렸지만 계속해서 에러 나다가 100,000으로 설정 후 RAM 용량 초과했는지 Crash 발생 후 Runtime 재시작됨... 중단."
      ],
      "metadata": {
        "id": "PQnVzHRdpbOV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypothesis 2. Strong vs. Weak Enhancer"
      ],
      "metadata": {
        "id": "kDdKLMyRl7aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_sw = model_sw.predict(X_test_embedded_sw)\n",
        "y_pred_sw = y_pred_sw.reshape((200,))"
      ],
      "metadata": {
        "id": "TG3hDreRpf1Y"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn2, sp2, acc2, mcc2 = evaluation_metrics(y_test_sw, y_pred_sw)\n",
        "print(f\"sn={sn2}\")\n",
        "print(f\"sp={sp2}\")\n",
        "print(f\"acc={acc2}\")\n",
        "print(f\"mcc={mcc2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDRMzSqDps-w",
        "outputId": "b05be253-58bf-4052-9f90-e1b237ba482c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos_num= 100\n",
            "neg_num= 100\n",
            "tp= 100\n",
            "tn= 60\n",
            "fn= 0\n",
            "fp= 40\n",
            "sn=1.0\n",
            "sp=0.6\n",
            "acc=0.8\n",
            "mcc=0.6546536707079772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 6. Alternative Model: Enhancer-LSTMAtt (Pre-Trained)"
      ],
      "metadata": {
        "id": "L9JJFVFJp1AW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture\n",
        "\n",
        "소스코드가 공개되어 있음. 변수명과 오류 나는 부분만 수정.\n",
        "- 논문 링크(open access): https://www.mdpi.com/2218-273X/12/7/995\n",
        "- 소스코드: https://github.com/feng-123/Enhancer-LSTMAtt"
      ],
      "metadata": {
        "id": "jXnwYFChz3Fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 이 모델에서는 Word2Vec 모델을 쓰지 않고 다른 방식으로 인코딩 후 모델에서 Embedding layer를 사용함.\n",
        "\n",
        "def encode_matrix(seq_matrix):\n",
        "  ind_to_char = ['A','T','C','G','N']\n",
        "  char_to_ind = {char: i for i, char in enumerate(ind_to_char)}\n",
        "  \n",
        "  return [[char_to_ind[i] for i in s] for s in seq_matrix]"
      ],
      "metadata": {
        "id": "aBURsK-Hp_zc"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_encoded_pn = encode_matrix(X_train_pn)\n",
        "X_test_encoded_pn = encode_matrix(X_test_pn)\n",
        "X_train_encoded_sw = encode_matrix(X_train_sw)\n",
        "X_test_encoded_sw = encode_matrix(X_test_sw)\n",
        "\n",
        "X_train_encoded_pn = np.array(X_train_encoded_pn)\n",
        "X_test_encoded_pn = np.array(X_test_encoded_pn)\n",
        "X_train_encoded_sw = np.array(X_train_encoded_sw)\n",
        "X_test_encoded_sw = np.array(X_test_encoded_sw)"
      ],
      "metadata": {
        "id": "-EMNzDcjqXfX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Embedding,Dense,Flatten,Dropout,Add,Bidirectional,LSTM,Conv1D,GlobalMaxPool1D,MaxPooling1D,BatchNormalization,Activation,Reshape"
      ],
      "metadata": {
        "id": "e-6wxcLBrywP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention3d(Layer):\n",
        "\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Attention mechanism for temporal data.\n",
        "        Supports Masking.\n",
        "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
        "        # Input shape\n",
        "            3D tensor with shape: `(samples, steps, features)`.\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "        Example:\n",
        "            # 1\n",
        "            model.add(LSTM(64, return_sequences=True))\n",
        "            model.add(Attention())\n",
        "            # next add a Dense layer (for classification/regression) or whatever...\n",
        "            # 2\n",
        "            hidden = LSTM(64, return_sequences=True)(words)\n",
        "            sentence = Attention()(hidden)\n",
        "            # next add a Dense layer (for classification/regression) or whatever...\n",
        "        \"\"\"\n",
        "        #self.supports_masking = True\n",
        "        \n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "\n",
        "        super(Attention3d, self).__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "         config = {\"W_regularizer\":self.W_regularizer,\n",
        "                   \"b_regularizer\":self.b_regularizer,\"W_constraint\":self.W_constraint,\"b_constraint\":self.b_constraint,\n",
        "                    \"bias\":self.bias,\"step_dim\":self.step_dim,\"features_dim\":self.features_dim}\n",
        "         base_config = super(Attention3d, self).get_config()\n",
        "         return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=initializers.get('glorot_uniform'),\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
        "        if self.bias:\n",
        "            e += self.b\n",
        "        e = K.tanh(e)\n",
        "\n",
        "        a = K.exp(e)\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        a = K.expand_dims(a)\n",
        "\n",
        "        c = K.sum(a * x, axis=1)\n",
        "        return c\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], self.features_dim"
      ],
      "metadata": {
        "id": "OAQHm_FGq6Ne"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet_identity_block(input_data, filters, kernel_size):\n",
        "    x = Conv1D(filters, kernel_size, strides=1, padding='same')(input_data)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv1D(filters, kernel_size, strides=1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, input_data])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def resnet_convolutional_block(input_data, filters, kernel_size):\n",
        "    x = Conv1D(filters, kernel_size, strides=2, padding='valid')(input_data)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv1D(filters, kernel_size, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    X = Conv1D(filters, kernel_size, strides=2, padding='valid')(input_data)\n",
        "    x = Add()([x, X])\n",
        "    x = Activation('relu')(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "jGPRmnddrfJi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model():\n",
        "    maxlen = 200\n",
        "    max_features = 5\n",
        "    embedding_dims = 32\n",
        "    class_num = 1\n",
        "    last_activation = 'sigmoid'\n",
        "    input = Input((maxlen,))\n",
        "    embedding = Embedding(max_features, embedding_dims, input_length=maxlen)(input)\n",
        "    y = Conv1D(32, 8, strides=1, padding='same')(embedding)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = MaxPooling1D(pool_size=2, strides=1)(y)\n",
        "    y = resnet_convolutional_block(y, 64, 8)\n",
        "    y = resnet_identity_block(y, 64, 8)\n",
        "    y = resnet_identity_block(y, 64, 8) \n",
        "    y = GlobalMaxPool1D()(y)\n",
        "\n",
        "    x = Bidirectional(LSTM(32, return_sequences=True))(embedding)  # LSTM\n",
        "    x = Bidirectional(LSTM(32, return_sequences=True))(x)\n",
        "    x = Attention3d(maxlen)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    t = tf.keras.layers.Concatenate()([x,y])\n",
        "    t = Dense(16,activation='relu')(t)\n",
        "    output = Dense(class_num, activation=last_activation)(t)\n",
        "    model = Model(inputs=input, outputs=output)\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                  metrics=['accuracy'])\n",
        "    ###원 코드에서 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    ###--> UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
        "    ###수정함\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Gppxy1I5rlMT"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Hypotheses\n",
        "\n",
        "교차검증 및 하이퍼파라미터 튜닝은 생략.\n",
        "\n",
        "*batch_size, epochs는 논문에 따로 명시가 안 되어 있음.*\n",
        "- 위의 모델링에서 best parameter로 나온 batch_size=8 사용.\n",
        "- epochs는 모델 성능 향상 정도를 보고 과적합 안 될 정도로 조정해서 학습."
      ],
      "metadata": {
        "id": "n2n9IPE20OHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis 1"
      ],
      "metadata": {
        "id": "vNEQ9AuZ1OXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_pn = define_model()\n",
        "model_pn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4ZJ17bAzF4W",
        "outputId": "96a0ab47-88eb-4043-a059-bba35a24bc32"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_14 (InputLayer)          [(None, 200)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_9 (Embedding)        (None, 200, 32)      160         ['input_14[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_80 (Conv1D)             (None, 200, 32)      8224        ['embedding_9[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_63 (BatchN  (None, 200, 32)     128         ['conv1d_80[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_71 (Activation)     (None, 200, 32)      0           ['batch_normalization_63[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling1d_9 (MaxPooling1D)  (None, 199, 32)     0           ['activation_71[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_81 (Conv1D)             (None, 96, 64)       16448       ['max_pooling1d_9[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_64 (BatchN  (None, 96, 64)      256         ['conv1d_81[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_72 (Activation)     (None, 96, 64)       0           ['batch_normalization_64[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_82 (Conv1D)             (None, 96, 64)       32832       ['activation_72[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_65 (BatchN  (None, 96, 64)      256         ['conv1d_82[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_83 (Conv1D)             (None, 96, 64)       16448       ['max_pooling1d_9[0][0]']        \n",
            "                                                                                                  \n",
            " add_27 (Add)                   (None, 96, 64)       0           ['batch_normalization_65[0][0]', \n",
            "                                                                  'conv1d_83[0][0]']              \n",
            "                                                                                                  \n",
            " activation_73 (Activation)     (None, 96, 64)       0           ['add_27[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_84 (Conv1D)             (None, 96, 64)       32832       ['activation_73[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_66 (BatchN  (None, 96, 64)      256         ['conv1d_84[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_74 (Activation)     (None, 96, 64)       0           ['batch_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_85 (Conv1D)             (None, 96, 64)       32832       ['activation_74[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_67 (BatchN  (None, 96, 64)      256         ['conv1d_85[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_28 (Add)                   (None, 96, 64)       0           ['batch_normalization_67[0][0]', \n",
            "                                                                  'activation_73[0][0]']          \n",
            "                                                                                                  \n",
            " activation_75 (Activation)     (None, 96, 64)       0           ['add_28[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_86 (Conv1D)             (None, 96, 64)       32832       ['activation_75[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_68 (BatchN  (None, 96, 64)      256         ['conv1d_86[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_76 (Activation)     (None, 96, 64)       0           ['batch_normalization_68[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_87 (Conv1D)             (None, 96, 64)       32832       ['activation_76[0][0]']          \n",
            "                                                                                                  \n",
            " bidirectional_18 (Bidirectiona  (None, 200, 64)     16640       ['embedding_9[0][0]']            \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_69 (BatchN  (None, 96, 64)      256         ['conv1d_87[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " bidirectional_19 (Bidirectiona  (None, 200, 64)     24832       ['bidirectional_18[0][0]']       \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " add_29 (Add)                   (None, 96, 64)       0           ['batch_normalization_69[0][0]', \n",
            "                                                                  'activation_75[0][0]']          \n",
            "                                                                                                  \n",
            " attention3d_9 (Attention3d)    (None, 64)           264         ['bidirectional_19[0][0]']       \n",
            "                                                                                                  \n",
            " activation_77 (Activation)     (None, 96, 64)       0           ['add_29[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 64)           0           ['attention3d_9[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_9 (Global  (None, 64)          0           ['activation_77[0][0]']          \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate)    (None, 128)          0           ['dropout_13[0][0]',             \n",
            "                                                                  'global_max_pooling1d_9[0][0]'] \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 16)           2064        ['concatenate_9[0][0]']          \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 1)            17          ['dense_22[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 250,921\n",
            "Trainable params: 250,089\n",
            "Non-trainable params: 832\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_pn = define_model()\n",
        "model_pn.fit(X_train_encoded_pn, y_train_pn, batch_size=8, epochs=5, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYWq1mFa3oOm",
        "outputId": "497970ed-fdf0-4d02-9741-79b6ed660168"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "371/371 [==============================] - 17s 30ms/step - loss: 0.6604 - accuracy: 0.6792\n",
            "Epoch 2/5\n",
            "371/371 [==============================] - 11s 30ms/step - loss: 0.4447 - accuracy: 0.7982\n",
            "Epoch 3/5\n",
            "371/371 [==============================] - 11s 30ms/step - loss: 0.3162 - accuracy: 0.8898\n",
            "Epoch 4/5\n",
            "371/371 [==============================] - 11s 29ms/step - loss: 0.2097 - accuracy: 0.9515\n",
            "Epoch 5/5\n",
            "371/371 [==============================] - 11s 30ms/step - loss: 0.1317 - accuracy: 0.9845\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff101a8dfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 소스코드 가져옴 (*y_pred 값이 확률값으로 나와서 조정을 해주는 코드)\n",
        "### 임계값 수치가 0.5, 0.6으로 잘못 나와 있어서 수정\n",
        "\n",
        "### 근데 왜 활성화함수가 시그모이드인데 확률값으로 나오지...?\n",
        "\n",
        "res1 = model_pn.predict(X_test_encoded_pn)\n",
        "pred1 = np.squeeze(res1, axis=-1)\n",
        "f1 = pred1>0.5\n",
        "pred1[f1] = 1\n",
        "pred1[pred1<=0.5] = 0\n",
        "\n",
        "y_pred_pn = pred1"
      ],
      "metadata": {
        "id": "pLCPAJNQsFSb"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn1, sp1, acc1, mcc1 = evaluation_metrics(y_test_pn, y_pred_pn)\n",
        "print(f\"sn={sn1}\")\n",
        "print(f\"sp={sp1}\")\n",
        "print(f\"acc={acc1}\")\n",
        "print(f\"mcc={mcc1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T59uUX6f4WHm",
        "outputId": "f855ef36-24db-451b-df11-84aaff242796"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos_num= 200\n",
            "neg_num= 200\n",
            "tp= 155\n",
            "tn= 162\n",
            "fn= 45\n",
            "fp= 38\n",
            "sn=0.775\n",
            "sp=0.81\n",
            "acc=0.7925\n",
            "mcc=0.5853586420360279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis 2"
      ],
      "metadata": {
        "id": "AK8QJS0z2bLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_sw = define_model()\n",
        "model_sw.fit(X_train_encoded_sw, y_train_sw, batch_size=8, epochs=5, verbose=1)"
      ],
      "metadata": {
        "id": "fZBdqmeXsALl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7bd22f3-3490-4a05-c1a2-0ee18c371f86"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "186/186 [==============================] - 14s 35ms/step - loss: 0.7786 - accuracy: 0.5499\n",
            "Epoch 2/5\n",
            "186/186 [==============================] - 10s 53ms/step - loss: 0.4740 - accuracy: 0.8275\n",
            "Epoch 3/5\n",
            "186/186 [==============================] - 8s 41ms/step - loss: 0.3197 - accuracy: 0.9589\n",
            "Epoch 4/5\n",
            "186/186 [==============================] - 7s 39ms/step - loss: 0.2225 - accuracy: 0.9892\n",
            "Epoch 5/5\n",
            "186/186 [==============================] - 6s 33ms/step - loss: 0.1594 - accuracy: 0.9980\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff10411f2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res2 = model_pn.predict(X_test_encoded_sw)\n",
        "pred2 = np.squeeze(res2, axis=-1)\n",
        "f2 = pred2>0.5\n",
        "pred2[f2] = 1\n",
        "pred2[pred2<=0.5] = 0\n",
        "\n",
        "y_pred_sw = pred2"
      ],
      "metadata": {
        "id": "OALytxEm2JhY"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn2, sp2, acc2, mcc2 = evaluation_metrics(y_test_sw, y_pred_sw)\n",
        "print(f\"sn={sn2}\")\n",
        "print(f\"sp={sp2}\")\n",
        "print(f\"acc={acc2}\")\n",
        "print(f\"mcc={mcc2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKKwk1sJ5zmJ",
        "outputId": "a0316873-5d27-43af-b7a9-94a5be18f5de"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos_num= 100\n",
            "neg_num= 100\n",
            "tp= 100\n",
            "tn= 37\n",
            "fn= 0\n",
            "fp= 63\n",
            "sn=1.0\n",
            "sp=0.37\n",
            "acc=0.685\n",
            "mcc=0.47643873166512696\n"
          ]
        }
      ]
    }
  ]
}